# 🧠 AI 6일차: 이미지 딥러닝 모델과 학습 전략

---

## 📘 1. 이미지 딥러닝 모델

### 1) CNN 살펴보기
**CNN(Convolutional Neural Network)** 은 이미지 인식에서 가장 널리 사용되는 신경망 모델입니다.  
이미지 데이터를 처리하는 데 매우 효율적이며, 필터를 사용하여 이미지에서 중요한 특성(feature)을 추출합니다.

#### 🔹 CNN vs FCN (Fully Connected Network)
- **CNN**: 이미지의 **공간적 구조**를 고려한 모델로, 필터를 이용해 이미지의 특성을 추출합니다.
- **FCN**: 모든 입력 노드가 각 출력 노드와 연결된 **완전 연결** 신경망으로, 이미지의 **위치 정보**를 고려하지 않습니다.

#### 🔹 모델 구조
- **Convolution Layer**: 이미지에서 특징을 추출
- **Pooling Layer**: 특징 맵을 다운샘플링하여 계산량 감소
- **Fully Connected Layer**: 추출된 특성들을 바탕으로 최종 분류

---

### 2) CNN 기반 모델 변천사

#### 🔹 AlexNet
- **2012년**: 이미지넷 대회에서 우수한 성능을 보여준 모델  
- **특징**: ReLU 활성화 함수, dropout 기법, 데이터 증강 등을 적용하여 성능을 높임

#### 🔹 VGGNet
- **2014년**: 심플한 구조로 깊은 네트워크를 사용하여 성능 향상  
- **특징**: 3x3 필터를 사용한 다층 구조로, 네트워크의 깊이를 크게 확장

#### 🔹 ResNet
- **2015년**: 잔차 학습(Residual Learning)을 통해 매우 깊은 네트워크 학습 가능  
- **특징**: **스킵 연결(skip connection)** 을 사용하여 기울기 소실 문제 해결

#### 🔹 MobileNet
- **2017년**: 모바일 및 임베디드 환경을 위한 경량화된 모델  
- **특징**: Depthwise Separable Convolution을 사용하여 모델 크기와 연산을 줄임

---

## 📗 2. 다양한 신경망 모델

### 1) CNN의 한계

#### 🔹 CNN의 구조 및 장점
- **장점**: 이미지에서 중요한 특징을 자동으로 추출하여 매우 효율적으로 작동
- **구조**: Convolutional layer와 pooling layer를 활용하여 이미지 특징을 추출

#### 🔹 CNN의 한계
- **한계**: CNN은 **고정된 크기의 필터**를 사용하여 공간 정보를 다루지만, 복잡한 시퀀스 정보나 긴 거리 의존성을 다루기 어려움

---

### 2) 시퀀스 데이터: RNN

#### 🔹 순차 데이터 처리 방식
- **RNN**(Recurrent Neural Network)은 **순차적** 데이터의 의존성을 처리하는 신경망으로, 이전 정보를 기억하여 현재 정보를 예측합니다.
  
#### 🔹 단순 RNN의 한계
- **기울기 소실(vanishing gradient)** 문제: 긴 시퀀스를 처리할 때, 역전파 과정에서 기울기가 작아져 학습이 어려움

#### 🔹 대안 모델
- **LSTM(Long Short-Term Memory)**: RNN의 한계를 극복하기 위해 개발된 모델로, 긴 거리 의존성을 더 잘 학습할 수 있습니다.

---

### 3) 긴 거리 의존성: 어텐션/ViT

#### 🔹 어텐션 메커니즘
- **어텐션**은 중요한 정보에 집중하고, 덜 중요한 정보를 무시하는 방식으로, 긴 시퀀스를 처리할 때 유용합니다.
- **Self-Attention**은 입력의 모든 부분에 대해 상대적인 중요도를 계산합니다.

#### 🔹 ViT (Vision Transformer)
- **ViT**는 이미지 데이터를 처리할 때 **Transformers**를 활용하는 모델입니다.  
- **위치 인코딩**을 통해 이미지의 **위치 정보를** 반영하고, 이미지를 패치 단위로 나누어 처리합니다.

---

## 📙 3. 이미지 모델 학습 전략

### 1) 학습 전략의 중요성

#### 🔹 학습 불안정, 과적합, 수렴 속도 문제
- **학습 불안정**: 모델 학습이 일정한 방향으로 수렴하지 않거나 발산할 때 발생
- **과적합**: 훈련 데이터에 너무 맞춰져 테스트 데이터에서 성능이 떨어짐
- **수렴 속도**: 학습이 너무 느리게 진행될 때, 적절한 학습 속도가 필요함

#### 🔹 실적용에서는 일반화 성능, 효율성이 핵심 -> "훈련 전략" 필요

---

### 2) 모델 구성: 악마는 디테일에...

#### 🔹 데이터 전처리/활성화 함수 선택
- **데이터 전처리**: 학습 데이터의 품질을 높이고 모델의 효율성을 극대화
- **활성화 함수**: **ReLU**, **Leaky ReLU** 등 다양한 활성화 함수 선택에 따라 모델 성능에 큰 영향을 미침

#### 🔹 가중치 초기화 전략/정규화 중요성
- **가중치 초기화**: 좋은 초기화 방법은 학습이 더 빠르고 안정적으로 이루어지게 함
- **정규화**: 모델이 과적합되는 것을 방지하고, 학습 안정성을 높임

---

### 3) 학습 과정

#### 🔹 학습률 조정/스케줄
- **학습률**(Learning Rate)은 모델이 최적화되는 속도에 영향을 미치므로 적절하게 설정해야 함.
- **학습률 스케줄링**: 학습 중에 학습률을 점진적으로 줄여서 최적화 과정을 더 효율적으로 만들 수 있음

#### 🔹 하이퍼 파라미터 최적화/데이터 증강
- **하이퍼 파라미터 최적화**: 모델을 최적화하기 위해 다양한 하이퍼 파라미터를 실험적으로 조정
- **데이터 증강**: 데이터를 인위적으로 늘려 모델의 일반화 능력을 향상시킬 수 있음

---

## 📚 요약

| 구분 | 주요 개념 | 핵심 포인트 |
|------|-----------|-------------|
| CNN | 합성곱 신경망 | 이미지 인식에 효과적 |
| RNN | 순차 데이터 처리 | 시간적 연속성 반영 |
| ViT | Vision Transformer | 이미지 데이터를 Transformer 방식으로 처리 |
| 학습 전략 | 모델 최적화 | 학습률, 정규화, 데이터 증강 등 중요 |

---

> 💡 **Tip:**  
> **어텐션 메커니즘**은 긴 거리 의존성을 처리하는 데 유리하여, 순차적 데이터 처리 시 큰 장점을 가집니다.
